{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db206658",
   "metadata": {},
   "outputs": [],
   "source": [
    "## linear regression model Numpy only\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "class LinearRegressionNP:\n",
    "    def __init__(self,num_features):\n",
    "        self.w = np.random.randn(num_features,1) * 0.01\n",
    "        self.b = np.zeros((1,))\n",
    "        \n",
    "    def forward(self,X):\n",
    "        return X @ self.w +self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "180a38d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELossNP:\n",
    "    def forward(self,y_hat,y):\n",
    "        return np.mean((y_hat -y)**2)\n",
    "    def backward(self,y_hat,y):\n",
    "        grad = 2 * (y_hat -y)/y.shape[0]\n",
    "        return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6152bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDNP:\n",
    "    def __init__(self, params, lr):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "\n",
    "    def step(self, grads):\n",
    "        for param, grad in zip(self.params, grads):\n",
    "            param -= self.lr * grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79946663",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_np(model, loss_fn, optimizer, X, y, epochs=100, batch_size=10):\n",
    "    n = X.shape[0]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        indices = np.random.permutation(n)\n",
    "        X, y = X[indices], y[indices]\n",
    "\n",
    "        for i in range(0, n, batch_size):\n",
    "            Xb = X[i:i+batch_size]\n",
    "            yb = y[i:i+batch_size]\n",
    "\n",
    "            # forward\n",
    "            y_hat = model.forward(Xb)\n",
    "            loss = loss_fn.forward(y_hat, yb)\n",
    "\n",
    "            # backward\n",
    "            grad_y_hat = loss_fn.backward(y_hat, yb)\n",
    "            grad_w = Xb.T @ grad_y_hat\n",
    "            grad_b = grad_y_hat.sum(axis=0)\n",
    "\n",
    "            optimizer.step([grad_w, grad_b])\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"[NumPy] Epoch {epoch+1}, Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9ef3c487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NumPy] Epoch 10, Loss: 0.0629\n",
      "[NumPy] Epoch 20, Loss: 0.0163\n",
      "[NumPy] Epoch 30, Loss: 0.0084\n",
      "[NumPy] Epoch 40, Loss: 0.0112\n",
      "[NumPy] Epoch 50, Loss: 0.0089\n",
      "[NumPy] Epoch 60, Loss: 0.0101\n",
      "[NumPy] Epoch 70, Loss: 0.0106\n",
      "[NumPy] Epoch 80, Loss: 0.0065\n",
      "[NumPy] Epoch 90, Loss: 0.0053\n",
      "[NumPy] Epoch 100, Loss: 0.0093\n",
      "Learned w: [[3.99369]] b: [2.02294907]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "X = np.random.rand(100, 1)\n",
    "y = 4 * X + 2 + 0.1 * np.random.randn(100, 1)\n",
    "\n",
    "model = LinearRegressionNP(1)\n",
    "loss_fn = MSELossNP()\n",
    "optimizer = SGDNP([model.w, model.b], lr=0.1)\n",
    "\n",
    "train_np(model, loss_fn, optimizer, X, y)\n",
    "print(\"Learned w:\", model.w, \"b:\", model.b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ddea6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
